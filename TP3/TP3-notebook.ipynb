{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning   </h1>\n",
    "<h1 style=\"text-align:center\"> Lab Session 3 - 3 Hours </h1>\n",
    "<h1 style=\"text-align:center\">Long Short Term Memory (LSTM) for Language Modeling</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Student 1:</b> Sofiene JERBI\n",
    " \n",
    " \n",
    "In this Lab Session,  you will build and train a Recurrent Neural Network, based on Long Short-Term Memory (LSTM) units for next word prediction task. \n",
    "\n",
    "Answers and experiments should be made by groups of one or two students. Each group should fill and run appropriate notebook cells. \n",
    "Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an pdf document using print as PDF (Ctrl+P). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed by June 9th 2017.\n",
    "\n",
    "Send you pdf file to benoit.huet@eurecom.fr and olfa.ben-ahmed@eurecom.fr using **[DeepLearning_lab3]** as Subject of your email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will train a LSTM to predict the next word using a sample short story. The LSTM will learn to predict the next item of a sentence from the 3 previous items (given as input). Ponctuation marks are considered as dictionnary items so they can be predicted too. Figure 1 shows the LSTM and the process of next word prediction. \n",
    "\n",
    "<img src=\"lstm.png\" height=\"370\" width=\"370\"> \n",
    "\n",
    "\n",
    "Each word (and ponctuation) from text sentences is encoded by a unique integer. The integer value corresponds to the index of the corresponding word (or punctuation mark) in the dictionnary. The network output is a one-hot-vector indicating the index of the predicted word in the reversed dictionnary (Section 1.2). For example if the prediction is 86, the predicted word will be \"company\". \n",
    "\n",
    "\n",
    "\n",
    "You will use a sample short story from Aesopâ€™s Fables (http://www.taleswithmorals.com/) to train your model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\" face=\"verdana\" > <i> \"There was once a young Shepherd Boy who tended his sheep at the foot of a mountain near a dark forest.\n",
    "\n",
    "It was rather lonely for him all day, so he thought upon a plan by which he could get a little company and some excitement.\n",
    "He rushed down towards the village calling out \"Wolf, Wolf,\" and the villagers came out to meet him, and some of them stopped with him for a considerable time.\n",
    "This pleased the boy so much that a few days afterwards he tried the same trick, and again the villagers came to his help.\n",
    "But shortly after this a Wolf actually did come out from the forest, and began to worry the sheep, and the boy of course cried out \"Wolf, Wolf,\" still louder than before.\n",
    "But this time the villagers, who had been fooled twice before, thought the boy was again deceiving them, and nobody stirred to come to his help.\n",
    "So the Wolf made a good meal off the boy's flock, and when the boy complained, the wise man of the village said:\n",
    "\"A liar will not be believed, even when he speaks the truth.\"  \"</i> </font>.    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading the necessary libraries and resetting the default computational graph. For more details about the rnn packages, we suggest you to take a look at https://www.tensorflow.org/api_guides/python/contrib.rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections # used to build the dictionary\n",
    "import random\n",
    "import time\n",
    "import pickle # may be used to save your model \n",
    "import matplotlib.pyplot as plt\n",
    "#Import Tensorflow and rnn\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn  \n",
    "\n",
    "# Target log path\n",
    "logs_path = 'lstm_words'\n",
    "writer = tf.summary.FileWriter(logs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next-word prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 1: Data  preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.1. Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and split the text of our story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there' 'was' 'once' 'a' 'young' 'shepherd' 'boy' 'who' 'tended' 'his'\n",
      " 'sheep' 'at' 'the' 'foot' 'of' 'a' 'mountain' 'near' 'a' 'dark' 'forest'\n",
      " '.' 'it' 'was' 'rather' 'lonely' 'for' 'him' 'all' 'day' ',' 'so' 'he'\n",
      " 'thought' 'upon' 'a' 'plan' 'by' 'which' 'he' 'could' 'get' 'a' 'little'\n",
      " 'company' 'and' 'some' 'excitement' '.' 'he' 'rushed' 'down' 'towards'\n",
      " 'the' 'village' 'calling' 'out' 'wolf' ',' 'wolf' ',' 'and' 'the'\n",
      " 'villagers' 'came' 'out' 'to' 'meet' 'him' ',' 'and' 'some' 'of' 'them'\n",
      " 'stopped' 'with' 'him' 'for' 'a' 'considerable' 'time' '.' 'this'\n",
      " 'pleased' 'the' 'boy' 'so' 'much' 'that' 'a' 'few' 'days' 'afterwards'\n",
      " 'he' 'tried' 'the' 'same' 'trick' ',' 'and' 'again' 'the' 'villagers'\n",
      " 'came' 'to' 'his' 'help' '.' 'but' 'shortly' 'after' 'this' 'a' 'wolf'\n",
      " 'actually' 'did' 'come' 'out' 'from' 'the' 'forest' ',' 'and' 'began' 'to'\n",
      " 'worry' 'the' 'sheep,' 'and' 'the' 'boy' 'of' 'course' 'cried' 'out'\n",
      " 'wolf' ',' 'wolf' ',' 'still' 'louder' 'than' 'before' '.' 'but' 'this'\n",
      " 'time' 'the' 'villagers' ',' 'who' 'had' 'been' 'fooled' 'twice' 'before'\n",
      " ',' 'thought' 'the' 'boy' 'was' 'again' 'deceiving' 'them' ',' 'and'\n",
      " 'nobody' 'stirred' 'to' 'come' 'to' 'his' 'help' '.' 'so' 'the' 'wolf'\n",
      " 'made' 'a' 'good' 'meal' 'off' 'the' \"boy's\" 'flock' ',' 'and' 'when'\n",
      " 'the' 'boy' 'complained' ',' 'the' 'wise' 'man' 'of' 'the' 'village'\n",
      " 'said' ':' 'a' 'liar' 'will' 'not' 'be' 'believed' ',' 'even' 'when' 'he'\n",
      " 'speaks' 'the' 'truth' '.']\n",
      "Loaded training data...\n",
      "214\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    with open(filename) as f:\n",
    "        data = f.readlines()\n",
    "    data = [x.strip().lower() for x in data]\n",
    "    data = [data[i].split() for i in range(len(data))]\n",
    "    data = np.array(data)\n",
    "    data = np.reshape(data, [-1, ])\n",
    "    print(data)\n",
    "    return data\n",
    "\n",
    "#Run the cell \n",
    "train_file ='data/story.txt'\n",
    "train_data = load_data(train_file)\n",
    "print(\"Loaded training data...\")\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.Symbols encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM input's can only be numbers. A way to convert words (symbols or any items) to numbers is to assign a unique integer to each word. This process is often based on frequency of occurrence for efficient coding purpose.\n",
    "\n",
    "Here, we define a function to build an indexed word dictionary (word->number). The \"build_vocabulary\" function builds both:\n",
    "\n",
    "- Dictionary : used for encoding words to numbers for the LSTM inputs \n",
    "- Reverted dictionnary : used for decoding the outputs of the LSTM into words (and punctuation).\n",
    "\n",
    "For example, in the story above, we have **113** individual words. The \"build_vocabulary\" function builds a dictionary with the following entries ['the': 0], [',': 1], ['company': 85],...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dic= dict()\n",
    "    for word, _ in count:\n",
    "        dic[word] = len(dic)\n",
    "    reverse_dic= dict(zip(dic.values(), dic.keys()))\n",
    "    return dic, reverse_dic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to display the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size (Vocabulary size) =  113\n",
      "\n",
      "\n",
      "Dictionary : \n",
      "\n",
      "{'but': 17, 'stirred': 32, 'days': 33, 'villagers': 11, 'forest': 25, 'cried': 35, 'calling': 36, 'he': 6, 'sheep,': 37, 'afterwards': 71, 'came': 19, 'plan': 38, 'deceiving': 39, 'of': 9, 'louder': 40, ',': 1, 'little': 41, 'get': 42, 'could': 43, 'tried': 44, 'before': 20, 'speaks': 45, 'did': 46, 'when': 21, 'nobody': 47, 'them': 22, 'had': 48, 'who': 23, 'course': 49, 'some': 24, 'once': 50, 'wolf': 5, 'shepherd': 52, 'meet': 104, 'even': 54, 'near': 34, 'tended': 55, 'stopped': 56, 'lonely': 57, 'by': 58, 'than': 59, 'rather': 60, 'excitement': 61, 'considerable': 62, 'still': 63, 'day': 64, 'shortly': 65, 'flock': 66, 'come': 26, 'was': 13, 'with': 67, 'from': 68, 'out': 10, 'made': 69, 'this': 14, 'same': 70, 'time': 18, 'help': 29, 'wise': 73, 'worry': 92, 'so': 15, 'which': 75, 'liar': 76, 'at': 77, 'been': 78, 'trick': 79, 'thought': 27, 'fooled': 80, 'a': 2, 'dark': 81, 'and': 3, 'rushed': 82, ':': 83, 'the': 0, 'off': 84, 'him': 12, 'again': 28, 'not': 86, 'be': 87, 'pleased': 88, 'believed': 51, 'foot': 89, 'mountain': 90, '.': 4, 'all': 91, 'good': 72, 'it': 74, 'company': 93, 'sheep': 94, 'began': 95, 'actually': 96, 'few': 97, 'his': 16, 'after': 85, 'twice': 98, 'will': 99, 'village': 30, 'to': 7, 'down': 112, 'much': 102, 'complained': 103, 'young': 53, 'for': 31, 'upon': 105, \"boy's\": 100, 'there': 107, 'said': 108, 'towards': 109, 'truth': 110, 'that': 106, 'boy': 8, 'meal': 111, 'man': 101}\n",
      "\n",
      "\n",
      "Reverted Dictionary : \n",
      "\n",
      "{0: 'the', 1: ',', 2: 'a', 3: 'and', 4: '.', 5: 'wolf', 6: 'he', 7: 'to', 8: 'boy', 9: 'of', 10: 'out', 11: 'villagers', 12: 'him', 13: 'was', 14: 'this', 15: 'so', 16: 'his', 17: 'but', 18: 'time', 19: 'came', 20: 'before', 21: 'when', 22: 'them', 23: 'who', 24: 'some', 25: 'forest', 26: 'come', 27: 'thought', 28: 'again', 29: 'help', 30: 'village', 31: 'for', 32: 'stirred', 33: 'days', 34: 'near', 35: 'cried', 36: 'calling', 37: 'sheep,', 38: 'plan', 39: 'deceiving', 40: 'louder', 41: 'little', 42: 'get', 43: 'could', 44: 'tried', 45: 'speaks', 46: 'did', 47: 'nobody', 48: 'had', 49: 'course', 50: 'once', 51: 'believed', 52: 'shepherd', 53: 'young', 54: 'even', 55: 'tended', 56: 'stopped', 57: 'lonely', 58: 'by', 59: 'than', 60: 'rather', 61: 'excitement', 62: 'considerable', 63: 'still', 64: 'day', 65: 'shortly', 66: 'flock', 67: 'with', 68: 'from', 69: 'made', 70: 'same', 71: 'afterwards', 72: 'good', 73: 'wise', 74: 'it', 75: 'which', 76: 'liar', 77: 'at', 78: 'been', 79: 'trick', 80: 'fooled', 81: 'dark', 82: 'rushed', 83: ':', 84: 'off', 85: 'after', 86: 'not', 87: 'be', 88: 'pleased', 89: 'foot', 90: 'mountain', 91: 'all', 92: 'worry', 93: 'company', 94: 'sheep', 95: 'began', 96: 'actually', 97: 'few', 98: 'twice', 99: 'will', 100: \"boy's\", 101: 'man', 102: 'much', 103: 'complained', 104: 'meet', 105: 'upon', 106: 'that', 107: 'there', 108: 'said', 109: 'towards', 110: 'truth', 111: 'meal', 112: 'down'}\n"
     ]
    }
   ],
   "source": [
    "dictionary, reverse_dictionary = build_vocabulary(train_data)\n",
    "vocabulary_size= len(dictionary) \n",
    "print(\"Dictionary size (Vocabulary size) = \", vocabulary_size)\n",
    "print(\"\\n\")\n",
    "print(\"Dictionary : \\n\")\n",
    "print(dictionary)\n",
    "print(\"\\n\")\n",
    "print(\"Reverted Dictionary : \\n\" )\n",
    "print(reverse_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : LSTM Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you have defined how the data will be modeled, you are now to develop an LSTM model to predict the word of following a sequence of 3 words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a 2-layers LSTM model.  \n",
    "\n",
    "For this use the following classes from the tensorflow.contrib library:\n",
    "\n",
    "- rnn.BasicLSTMCell(number of hidden units) \n",
    "- rnn.static_rnn(rnn_cell, data, dtype=tf.float32)\n",
    "- rnn.MultiRNNCell(,)\n",
    "\n",
    "\n",
    "You may need some tensorflow functions (https://www.tensorflow.org/api_docs/python/tf/) :\n",
    "- tf.split\n",
    "- tf.reshape \n",
    "- ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_model(x, w, b):\n",
    "    inputs = tf.reshape(x, [-1, n_input])\n",
    "    inputs = tf.split(inputs,n_input,1)\n",
    "    lstm = rnn.BasicLSTMCell(n_hidden)\n",
    "    stacked_lstm = rnn.MultiRNNCell([lstm]*2)\n",
    "    outputs, state = rnn.static_rnn(stacked_lstm, inputs, dtype=tf.float32)\n",
    "    model = tf.matmul(outputs[-1], w['out']) + b['out']\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Parameters and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 5000\n",
    "display_step = 100\n",
    "n_input = 3\n",
    "\n",
    "#For each LSTM cell that you initialise, supply a value for the hidden dimension, number of units in LSTM cell\n",
    "n_hidden = 64\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocabulary_size])\n",
    "\n",
    "# LSTM  weights and biases\n",
    "weights = { 'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([vocabulary_size])) }\n",
    "\n",
    "\n",
    "#build the model\n",
    "pred = lstm_model(x, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Loss/Cost and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We give you here the Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run the cell\n",
    "def test(sentence, session, verbose=False):\n",
    "    sentence = sentence.strip()\n",
    "    words = sentence.split(' ')\n",
    "    if len(words) != n_input:\n",
    "        print(\"sentence length should be equel to\", n_input, \"!\")\n",
    "    try:\n",
    "        symbols_inputs = [dictionary[str(words[i - n_input])] for i in range(n_input)]\n",
    "        keys = np.reshape(np.array(symbols_inputs), [-1, n_input, 1])\n",
    "        onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "        onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "        words.append(reverse_dictionary[onehot_pred_index])\n",
    "        sentence = \" \".join(words)\n",
    "        if verbose:\n",
    "            print(sentence)\n",
    "        return reverse_dictionary[onehot_pred_index]\n",
    "    except:\n",
    "        print(\" \".join([\"Word\", words[i - n_input], \"not in dictionary\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : LSTM Training  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Training process, at each epoch, 3 words are taken from the training data, encoded to integer to form the input vector. The training labels are one-hot vector encoding the word that comes after the 3 inputs words. Display the loss and the training accuracy every 1000 iteration. Save the model at the end of training in the **lstm_model** folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Epoch:  100   =====> Loss= 0.147865921   Validation Accuracy= 0.600000024\n",
      "Epoch:  200   =====> Loss= 0.170389204   Validation Accuracy= 0.738095224\n",
      "Epoch:  300   =====> Loss= 0.168773343   Validation Accuracy= 0.819047630\n",
      "Epoch:  400   =====> Loss= 0.174424479   Validation Accuracy= 0.933333337\n",
      "Epoch:  500   =====> Loss= 0.115744019   Validation Accuracy= 0.933333337\n",
      "Epoch:  600   =====> Loss= 0.104514964   Validation Accuracy= 0.966666639\n",
      "Epoch:  700   =====> Loss= 0.113949627   Validation Accuracy= 0.961904764\n",
      "Epoch:  800   =====> Loss= 0.101774005   Validation Accuracy= 0.942857146\n",
      "Epoch:  900   =====> Loss= 0.096509059   Validation Accuracy= 0.909523785\n",
      "Epoch:  1000   =====> Loss= 0.099715668   Validation Accuracy= 0.966666639\n",
      "Epoch:  1100   =====> Loss= 0.120024360   Validation Accuracy= 0.966666639\n",
      "Epoch:  1200   =====> Loss= 0.064044490   Validation Accuracy= 0.966666639\n",
      "Epoch:  1300   =====> Loss= 0.068750653   Validation Accuracy= 0.947619021\n",
      "Epoch:  1400   =====> Loss= 0.074889988   Validation Accuracy= 0.947619021\n",
      "Epoch:  1500   =====> Loss= 0.064591543   Validation Accuracy= 0.928571403\n",
      "Epoch:  1600   =====> Loss= 0.078585243   Validation Accuracy= 0.933333337\n",
      "Epoch:  1700   =====> Loss= 0.066757272   Validation Accuracy= 0.957142830\n",
      "Epoch:  1800   =====> Loss= 0.107667904   Validation Accuracy= 0.966666639\n",
      "Epoch:  1900   =====> Loss= 0.101936754   Validation Accuracy= 0.957142830\n",
      "Epoch:  2000   =====> Loss= 0.134513525   Validation Accuracy= 0.942857146\n",
      "Epoch:  2100   =====> Loss= 0.067714199   Validation Accuracy= 0.952380955\n",
      "Epoch:  2200   =====> Loss= 0.068993106   Validation Accuracy= 0.933333337\n",
      "Epoch:  2300   =====> Loss= 0.084536250   Validation Accuracy= 0.938095212\n",
      "Epoch:  2400   =====> Loss= 0.094430442   Validation Accuracy= 0.971428573\n",
      "Epoch:  2500   =====> Loss= 0.059607406   Validation Accuracy= 0.966666639\n",
      "Epoch:  2600   =====> Loss= 0.074727170   Validation Accuracy= 0.966666639\n",
      "Epoch:  2700   =====> Loss= 0.071773078   Validation Accuracy= 0.938095212\n",
      "Epoch:  2800   =====> Loss= 0.051852476   Validation Accuracy= 0.966666639\n",
      "Epoch:  2900   =====> Loss= 0.065605866   Validation Accuracy= 0.952380955\n",
      "Epoch:  3000   =====> Loss= 0.062923830   Validation Accuracy= 0.961904764\n",
      "Epoch:  3100   =====> Loss= 0.049458872   Validation Accuracy= 0.966666639\n",
      "Epoch:  3200   =====> Loss= 0.078504876   Validation Accuracy= 0.961904764\n",
      "Epoch:  3300   =====> Loss= 0.075386213   Validation Accuracy= 0.952380955\n",
      "Epoch:  3400   =====> Loss= 0.041703840   Validation Accuracy= 0.971428573\n",
      "Epoch:  3500   =====> Loss= 0.042845455   Validation Accuracy= 0.971428573\n",
      "Epoch:  3600   =====> Loss= 0.065092187   Validation Accuracy= 0.980952382\n",
      "Epoch:  3700   =====> Loss= 0.040171774   Validation Accuracy= 0.961904764\n",
      "Epoch:  3800   =====> Loss= 0.042057558   Validation Accuracy= 0.976190448\n",
      "Epoch:  3900   =====> Loss= 0.044503787   Validation Accuracy= 0.980952382\n",
      "Epoch:  4000   =====> Loss= 0.119344747   Validation Accuracy= 0.976190448\n",
      "Epoch:  4100   =====> Loss= 0.045276684   Validation Accuracy= 0.957142830\n",
      "Epoch:  4200   =====> Loss= 0.052740845   Validation Accuracy= 0.961904764\n",
      "Epoch:  4300   =====> Loss= 0.051995155   Validation Accuracy= 0.976190448\n",
      "Epoch:  4400   =====> Loss= 0.059365038   Validation Accuracy= 0.976190448\n",
      "Epoch:  4500   =====> Loss= 0.087006721   Validation Accuracy= 0.928571403\n",
      "Epoch:  4600   =====> Loss= 0.037684478   Validation Accuracy= 0.966666639\n",
      "Epoch:  4700   =====> Loss= 0.059595315   Validation Accuracy= 0.980952382\n",
      "Epoch:  4800   =====> Loss= 0.111216326   Validation Accuracy= 0.976190448\n",
      "Epoch:  4900   =====> Loss= 0.064248243   Validation Accuracy= 0.966666639\n",
      "Epoch:  5000   =====> Loss= 0.048487203   Validation Accuracy= 0.942857146\n",
      "End Of training Finished!\n",
      "time:  1722.6179084777832\n",
      "For tensorboard visualisation run on command line.\n",
      "\ttensorboard --logdir=lstm_words\n",
      "and point your web browser to the returned link\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "start_time = time.time()\n",
    "init = tf.global_variables_initializer()\n",
    "model_saver = tf.train.Saver()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "# Initializing the session \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    print(\"Start Training\")\n",
    "    ##############################################\n",
    "    total_batch = len(train_data)-n_input-1\n",
    "    xs = []\n",
    "    ys = []\n",
    "    yt = np.zeros(vocabulary_size)\n",
    "    for i in range(total_batch):\n",
    "        xs += [np.array([dictionary[word] for word in train_data[i:i+n_input]]).reshape(1,n_input,1)]\n",
    "        idx = dictionary[train_data[i+n_input]]\n",
    "        yt[idx] = 1\n",
    "        ys += [np.copy(yt).reshape(1,vocabulary_size)]\n",
    "        yt[idx] = 0\n",
    "    xs_bis = np.array(xs).reshape(-1,n_input,1)\n",
    "    ys_bis = np.array(ys).reshape(-1,vocabulary_size)\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0.\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = xs[i], ys[i]\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary, onehot_pred = sess.run([optimizer, cost, merged_summary_op, pred], feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost), \"  Validation Accuracy=\", \"{:.9f}\".format(accuracy.eval(feed_dict={x: xs_bis, y: ys_bis})))\n",
    "\n",
    "    ##############################################\n",
    "    print(\"End Of training Finished!\")\n",
    "    print(\"time: \",time.time() - start_time)\n",
    "    print(\"For tensorboard visualisation run on command line.\")\n",
    "    print(\"\\ttensorboard --logdir=%s\" % (logs_path))\n",
    "    print(\"and point your web browser to the returned link\")\n",
    "    ##############################################\n",
    "    save_path = model_saver.save(sess, \"lstm_model/model\"+str(int(time.time())))\n",
    "    ##############################################\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of our training we see that we get a good training accuracy, close to 97%. The training worked well and fast.<br/>\n",
    "<img src=\"graphs/accuracy_3.png\"> <br/>\n",
    "<img src=\"graphs/loss_3.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 4 : Test your model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.1. Next word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your model (using the model_saved variable given in the training session) and test the sentences :\n",
    "- 'get a little' \n",
    "- 'nobody tried to'\n",
    "- Try with other sentences using words from the stroy's vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "get a little\n",
      "company\n",
      "**********\n",
      "nobody tried to\n",
      "come\n",
      "**********\n",
      "he speaks the\n",
      "truth\n",
      "**********\n",
      "the boy was\n",
      "tended\n",
      "**********\n",
      "the man came\n",
      "out\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    n_input = 3\n",
    "    x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "    weights = { 'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size]))}\n",
    "    biases = {'out': tf.Variable(tf.random_normal([vocabulary_size])) }\n",
    "    pred = lstm_model(x, weights, biases)\n",
    "    model_saver = tf.train.Saver()\n",
    "    init = tf.global_variables_initializer()\n",
    "    # Restore variables from disk.\n",
    "    model_saver.restore(sess, \"lstm_model/model1496327640\")\n",
    "    print(\"Model restored.\")\n",
    "    # Do some work with the model\n",
    "    for sentence in [\"get a little\",\"nobody tried to\", \"he speaks the\", \"the boy was\", \"the man came\"]:\n",
    "        print(sentence)\n",
    "        print(test(sentence, sess, verbose=False))\n",
    "        print(\"**********\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. More fun with the Fable Writer !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the RNN/LSTM model learned in the previous question to create a\n",
    "new story/fable.\n",
    "For this you will choose 3 words from the dictionary which will start your\n",
    "story and initialize your network. Using those 3 words the RNN will generate\n",
    "the next word or the story. Using the last 3 words (the newly predicted one\n",
    "and the last 2 from the input) you will use the network to predict the 5\n",
    "word of the story.. and so on until your story is 5 sentence long. \n",
    "Make a point at the end of your story. \n",
    "To implement that, you will use the test function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The boy was again deceiving them, and nobody stirred to come to his help. So the wolf made a good by which he could get a little company and some excitement. He rushed down towards the village calling out wolf, wolf, and the villagers came to his help. So the wolf made a good by which he could get a little company and some excitement. He rushed down towards the village calling out wolf, wolf, and the villagers came to his help.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    model_saver.restore(sess, \"lstm_model/model1496327640\")\n",
    "    fable = \"The boy was\"\n",
    "    nwords = \"the boy was\"\n",
    "    nbpoints = 0\n",
    "    while nbpoints<5:\n",
    "        word = test(nwords, sess, verbose=False)\n",
    "        if word in [\",\", \".\"]:\n",
    "            fable+= word\n",
    "        elif fable[-1]==\".\":\n",
    "            fable+= \" \" + word.title()\n",
    "        else:\n",
    "            fable+= \" \" + word\n",
    "        tmp = nwords.split(\" \")[1:] + [word]\n",
    "        nwords = \"\"\n",
    "        for w in tmp:\n",
    "            nwords += w + \" \"\n",
    "        nwords = nwords[:-1]\n",
    "        if word==\".\":\n",
    "            nbpoints+=1\n",
    "    print(fable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the created fable makes some sense (at least is written with a correct grammar) even though we enter a two sentences loop with the given 3-words input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Play with number of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The number of input in our example is 3, see what happens when you use other number (1 and 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Epoch:  100   =====> Loss= 3.653317738   Validation Accuracy= 0.132075474\n",
      "Epoch:  200   =====> Loss= 3.521692361   Validation Accuracy= 0.136792451\n",
      "Epoch:  300   =====> Loss= 3.468380347   Validation Accuracy= 0.132075474\n",
      "Epoch:  400   =====> Loss= 3.457010381   Validation Accuracy= 0.127358496\n",
      "Epoch:  500   =====> Loss= 3.408037501   Validation Accuracy= 0.113207549\n",
      "Epoch:  600   =====> Loss= 3.365570235   Validation Accuracy= 0.165094346\n",
      "Epoch:  700   =====> Loss= 3.302691246   Validation Accuracy= 0.127358496\n",
      "Epoch:  800   =====> Loss= 3.356638100   Validation Accuracy= 0.136792451\n",
      "Epoch:  900   =====> Loss= 3.468619726   Validation Accuracy= 0.146226421\n",
      "Epoch:  1000   =====> Loss= 3.486100896   Validation Accuracy= 0.160377353\n",
      "Epoch:  1100   =====> Loss= 3.474109301   Validation Accuracy= 0.146226421\n",
      "Epoch:  1200   =====> Loss= 3.360642204   Validation Accuracy= 0.146226421\n",
      "Epoch:  1300   =====> Loss= 3.585799253   Validation Accuracy= 0.165094346\n",
      "Epoch:  1400   =====> Loss= 3.603628030   Validation Accuracy= 0.165094346\n",
      "Epoch:  1500   =====> Loss= 3.632093321   Validation Accuracy= 0.179245278\n",
      "Epoch:  1600   =====> Loss= 3.359892724   Validation Accuracy= 0.150943398\n",
      "Epoch:  1700   =====> Loss= 3.523664069   Validation Accuracy= 0.136792451\n",
      "Epoch:  1800   =====> Loss= 3.706955627   Validation Accuracy= 0.146226421\n",
      "Epoch:  1900   =====> Loss= 3.582511196   Validation Accuracy= 0.146226421\n",
      "Epoch:  2000   =====> Loss= 3.560909204   Validation Accuracy= 0.141509429\n",
      "Epoch:  2100   =====> Loss= 3.524177737   Validation Accuracy= 0.136792451\n",
      "Epoch:  2200   =====> Loss= 3.620676091   Validation Accuracy= 0.160377353\n",
      "Epoch:  2300   =====> Loss= 3.641784786   Validation Accuracy= 0.122641511\n",
      "Epoch:  2400   =====> Loss= 3.448060605   Validation Accuracy= 0.136792451\n",
      "Epoch:  2500   =====> Loss= 3.626077445   Validation Accuracy= 0.122641511\n",
      "Epoch:  2600   =====> Loss= 3.748676876   Validation Accuracy= 0.160377353\n",
      "Epoch:  2700   =====> Loss= 3.815426432   Validation Accuracy= 0.150943398\n",
      "Epoch:  2800   =====> Loss= 3.823080017   Validation Accuracy= 0.160377353\n",
      "Epoch:  2900   =====> Loss= 3.831297997   Validation Accuracy= 0.150943398\n",
      "Epoch:  3000   =====> Loss= 3.910000080   Validation Accuracy= 0.165094346\n",
      "Epoch:  3100   =====> Loss= 4.004401170   Validation Accuracy= 0.160377353\n",
      "Epoch:  3200   =====> Loss= 4.077434455   Validation Accuracy= 0.146226421\n",
      "Epoch:  3300   =====> Loss= 3.969314514   Validation Accuracy= 0.141509429\n",
      "Epoch:  3400   =====> Loss= 4.041539134   Validation Accuracy= 0.117924526\n",
      "Epoch:  3500   =====> Loss= 4.030335814   Validation Accuracy= 0.141509429\n",
      "Epoch:  3600   =====> Loss= 4.295080918   Validation Accuracy= 0.127358496\n",
      "Epoch:  3700   =====> Loss= 4.107482737   Validation Accuracy= 0.141509429\n",
      "Epoch:  3800   =====> Loss= 4.119396010   Validation Accuracy= 0.141509429\n",
      "Epoch:  3900   =====> Loss= 4.091013938   Validation Accuracy= 0.141509429\n",
      "Epoch:  4000   =====> Loss= 4.350520946   Validation Accuracy= 0.136792451\n",
      "Epoch:  4100   =====> Loss= 4.363573014   Validation Accuracy= 0.141509429\n",
      "Epoch:  4200   =====> Loss= 4.428262399   Validation Accuracy= 0.165094346\n",
      "Epoch:  4300   =====> Loss= 4.270369068   Validation Accuracy= 0.146226421\n",
      "Epoch:  4400   =====> Loss= 4.245359859   Validation Accuracy= 0.122641511\n",
      "Epoch:  4500   =====> Loss= 4.132802029   Validation Accuracy= 0.127358496\n",
      "Epoch:  4600   =====> Loss= 4.308572468   Validation Accuracy= 0.146226421\n",
      "Epoch:  4700   =====> Loss= 4.324455579   Validation Accuracy= 0.141509429\n",
      "Epoch:  4800   =====> Loss= 4.207470364   Validation Accuracy= 0.141509429\n",
      "Epoch:  4900   =====> Loss= 4.381959259   Validation Accuracy= 0.150943398\n",
      "Epoch:  5000   =====> Loss= 4.104154845   Validation Accuracy= 0.150943398\n",
      "End Of training Finished!\n",
      "time:  1400.0792734622955\n",
      "For tensorboard visualisation run on command line.\n",
      "\ttensorboard --logdir=lstm_words\n",
      "and point your web browser to the returned link\n",
      "Model saved\n",
      "Start Training\n",
      "Epoch:  100   =====> Loss= 0.116170148   Validation Accuracy= 0.980769217\n",
      "Epoch:  200   =====> Loss= 0.036867332   Validation Accuracy= 0.985576928\n",
      "Epoch:  300   =====> Loss= 0.033989074   Validation Accuracy= 0.985576928\n",
      "Epoch:  400   =====> Loss= 0.028127590   Validation Accuracy= 0.990384638\n",
      "Epoch:  500   =====> Loss= 0.022575625   Validation Accuracy= 0.995192289\n",
      "Epoch:  600   =====> Loss= 0.017307184   Validation Accuracy= 0.995192289\n",
      "Epoch:  700   =====> Loss= 0.011622601   Validation Accuracy= 0.995192289\n",
      "Epoch:  800   =====> Loss= 0.008914488   Validation Accuracy= 0.995192289\n",
      "Epoch:  900   =====> Loss= 0.008800768   Validation Accuracy= 0.995192289\n",
      "Epoch:  1000   =====> Loss= 0.009632896   Validation Accuracy= 0.995192289\n",
      "Epoch:  1100   =====> Loss= 0.010444860   Validation Accuracy= 0.995192289\n",
      "Epoch:  1200   =====> Loss= 0.008359368   Validation Accuracy= 0.995192289\n",
      "Epoch:  1300   =====> Loss= 0.008450584   Validation Accuracy= 0.995192289\n",
      "Epoch:  1400   =====> Loss= 0.008140609   Validation Accuracy= 0.995192289\n",
      "Epoch:  1500   =====> Loss= 0.008127693   Validation Accuracy= 0.995192289\n",
      "Epoch:  1600   =====> Loss= 0.008104215   Validation Accuracy= 0.995192289\n",
      "Epoch:  1700   =====> Loss= 0.008293721   Validation Accuracy= 0.995192289\n",
      "Epoch:  1800   =====> Loss= 0.007858165   Validation Accuracy= 0.995192289\n",
      "Epoch:  1900   =====> Loss= 0.007806774   Validation Accuracy= 0.995192289\n",
      "Epoch:  2000   =====> Loss= 0.007681423   Validation Accuracy= 0.995192289\n",
      "Epoch:  2100   =====> Loss= 0.007669624   Validation Accuracy= 0.995192289\n",
      "Epoch:  2200   =====> Loss= 0.007604885   Validation Accuracy= 0.995192289\n",
      "Epoch:  2300   =====> Loss= 0.007728590   Validation Accuracy= 0.995192289\n",
      "Epoch:  2400   =====> Loss= 0.007740887   Validation Accuracy= 0.995192289\n",
      "Epoch:  2500   =====> Loss= 0.007896555   Validation Accuracy= 0.995192289\n",
      "Epoch:  2600   =====> Loss= 0.007761163   Validation Accuracy= 0.995192289\n",
      "Epoch:  2700   =====> Loss= 0.007812485   Validation Accuracy= 0.995192289\n",
      "Epoch:  2800   =====> Loss= 0.007742453   Validation Accuracy= 0.995192289\n",
      "Epoch:  2900   =====> Loss= 0.007773161   Validation Accuracy= 0.995192289\n",
      "Epoch:  3000   =====> Loss= 0.007905350   Validation Accuracy= 0.995192289\n",
      "Epoch:  3100   =====> Loss= 0.007975400   Validation Accuracy= 0.995192289\n",
      "Epoch:  3200   =====> Loss= 0.007943775   Validation Accuracy= 0.995192289\n",
      "Epoch:  3300   =====> Loss= 0.007943808   Validation Accuracy= 0.995192289\n",
      "Epoch:  3400   =====> Loss= 0.007830635   Validation Accuracy= 0.995192289\n",
      "Epoch:  3500   =====> Loss= 0.007597469   Validation Accuracy= 0.995192289\n",
      "Epoch:  3600   =====> Loss= 0.049600585   Validation Accuracy= 0.995192289\n",
      "Epoch:  3700   =====> Loss= 0.007599130   Validation Accuracy= 0.995192289\n",
      "Epoch:  3800   =====> Loss= 0.007548382   Validation Accuracy= 0.995192289\n",
      "Epoch:  3900   =====> Loss= 0.007469859   Validation Accuracy= 0.995192289\n",
      "Epoch:  4000   =====> Loss= 0.007427510   Validation Accuracy= 0.995192289\n",
      "Epoch:  4100   =====> Loss= 0.007386418   Validation Accuracy= 0.995192289\n",
      "Epoch:  4200   =====> Loss= 0.007347220   Validation Accuracy= 0.995192289\n",
      "Epoch:  4300   =====> Loss= 0.007336114   Validation Accuracy= 0.995192289\n",
      "Epoch:  4400   =====> Loss= 0.007339439   Validation Accuracy= 0.995192289\n",
      "Epoch:  4500   =====> Loss= 0.044567528   Validation Accuracy= 0.995192289\n",
      "Epoch:  4600   =====> Loss= 0.007399862   Validation Accuracy= 0.995192289\n",
      "Epoch:  4700   =====> Loss= 0.007583068   Validation Accuracy= 0.995192289\n",
      "Epoch:  4800   =====> Loss= 0.007302903   Validation Accuracy= 0.995192289\n",
      "Epoch:  4900   =====> Loss= 0.007271516   Validation Accuracy= 0.995192289\n",
      "Epoch:  5000   =====> Loss= 0.007398305   Validation Accuracy= 0.995192289\n",
      "End Of training Finished!\n",
      "time:  2078.5474016666412\n",
      "For tensorboard visualisation run on command line.\n",
      "\ttensorboard --logdir=lstm_words\n",
      "and point your web browser to the returned link\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "for n_input in [1,5]:\n",
    "    tf.reset_default_graph()\n",
    "    x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "    y = tf.placeholder(\"float\", [None, vocabulary_size])\n",
    "    weights = { 'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size]))}\n",
    "    biases = {'out': tf.Variable(tf.random_normal([vocabulary_size])) }\n",
    "    pred = lstm_model(x, weights, biases)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "    # Model evaluation\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initializing the variables\n",
    "    start_time = time.time()\n",
    "    init = tf.global_variables_initializer()\n",
    "    model_saver = tf.train.Saver()\n",
    "    # Create a summary to monitor cost tensor\n",
    "    tf.summary.scalar(\"Loss\", cost)\n",
    "    # Create a summary to monitor accuracy tensor\n",
    "    tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    # Merge all summaries into a single op\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    # Initializing the session \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        # op to write logs to Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        print(\"Start Training\")\n",
    "        ##############################################\n",
    "        total_batch = len(train_data)-n_input-1\n",
    "        xs = []\n",
    "        ys = []\n",
    "        yt = np.zeros(vocabulary_size)\n",
    "        for i in range(total_batch):\n",
    "            xs += [np.array([dictionary[word] for word in train_data[i:i+n_input]]).reshape(1,n_input,1)]\n",
    "            idx = dictionary[train_data[i+n_input]]\n",
    "            yt[idx] = 1\n",
    "            ys += [np.copy(yt).reshape(1,vocabulary_size)]\n",
    "            yt[idx] = 0\n",
    "        xs_bis = np.array(xs).reshape(-1,n_input,1)\n",
    "        ys_bis = np.array(ys).reshape(-1,vocabulary_size)\n",
    "        for epoch in range(epochs):\n",
    "            avg_cost = 0.\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = xs[i], ys[i]\n",
    "                # Run optimization op (backprop), cost op (to get loss value)\n",
    "                # and summary nodes\n",
    "                _, c, summary, onehot_pred = sess.run([optimizer, cost, merged_summary_op, pred], feed_dict={x: batch_xs, y: batch_ys})\n",
    "                # Write logs at every iteration\n",
    "                summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "            # Display logs per epoch step\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost), \"  Validation Accuracy=\", \"{:.9f}\".format(accuracy.eval(feed_dict={x: xs_bis, y: ys_bis})))\n",
    "\n",
    "        ##############################################\n",
    "        print(\"End Of training Finished!\")\n",
    "        print(\"time: \",time.time() - start_time)\n",
    "        print(\"For tensorboard visualisation run on command line.\")\n",
    "        print(\"\\ttensorboard --logdir=%s\" % (logs_path))\n",
    "        print(\"and point your web browser to the returned link\")\n",
    "        ##############################################\n",
    "        model_name = \"model\"+str(int(time.time()))\n",
    "        save_path = model_saver.save(sess, \"lstm_model/\"+model_name)\n",
    "        ##############################################\n",
    "        print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "help the\n",
      "the boy\n",
      "boy who\n",
      "who the\n",
      "the boy\n",
      "boy who\n",
      "who the\n",
      "the boy\n",
      "boy who\n",
      "who the\n",
      "Help the boy who the boy who the boy who the\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    n_input = 1\n",
    "    x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "    weights = { 'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size]))}\n",
    "    biases = {'out': tf.Variable(tf.random_normal([vocabulary_size])) }\n",
    "    pred = lstm_model(x, weights, biases)\n",
    "    model_saver = tf.train.Saver()\n",
    "    init = tf.global_variables_initializer()\n",
    "    model_saver.restore(sess, \"lstm_model/model1496341229\")\n",
    "    fable = \"Help\"\n",
    "    nwords = \"help\"\n",
    "    nbpoints = 0\n",
    "    it=0\n",
    "    while nbpoints<5 and it<10:\n",
    "        it+=1\n",
    "        word = test(nwords, sess, verbose=False)\n",
    "        print(nwords, word)\n",
    "        if word in [\",\", \".\"]:\n",
    "            fable+= word\n",
    "        elif fable[-1]==\".\":\n",
    "            fable+= \" \" + word.title()\n",
    "        else:\n",
    "            fable+= \" \" + word\n",
    "        nwords = word\n",
    "        if word==\".\":\n",
    "            nbpoints+=1\n",
    "    print(fable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The boy was a shepherd a and some again deceiving, so the the and the this a villagers complained been come out from day, so he thought upon a plan by which he could get a little company and some excitement. He rushed down towards the village calling out wolf, wolf, and the villagers came out to meet him, and some of them stopped with him for a considerable time. This pleased the boy so much that a few days afterwards he tried the same trick, and again the villagers came to his help. But shortly after this a wolf actually did come out from the forest, and began to worry the sheep, and the boy of course cried out wolf, wolf, and the villagers came out to meet him, and some of them stopped with him for a considerable time. This pleased the boy so much that a few days afterwards he tried the same trick, and again the villagers came to his help.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    n_input = 5\n",
    "    x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "    weights = { 'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size]))}\n",
    "    biases = {'out': tf.Variable(tf.random_normal([vocabulary_size])) }\n",
    "    pred = lstm_model(x, weights, biases)\n",
    "    model_saver = tf.train.Saver()\n",
    "    init = tf.global_variables_initializer()\n",
    "    model_saver.restore(sess, \"lstm_model/model1496343309\")\n",
    "    fable = \"The boy was a shepherd\"\n",
    "    nwords = \"the boy was a shepherd\"\n",
    "    nbpoints = 0\n",
    "    while nbpoints<5:\n",
    "        word = test(nwords, sess, verbose=False)\n",
    "        if word in [\",\", \".\"]:\n",
    "            fable+= word\n",
    "        elif fable[-1]==\".\":\n",
    "            fable+= \" \" + word.title()\n",
    "        else:\n",
    "            fable+= \" \" + word\n",
    "        tmp = nwords.split(\" \")[1:] + [word]\n",
    "        nwords = \"\"\n",
    "        for w in tmp:\n",
    "            nwords += w + \" \"\n",
    "        nwords = nwords[:-1]\n",
    "        if word==\".\":\n",
    "            nbpoints+=1\n",
    "    print(fable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, training with 1 word input gives underfitting, the validation accuracy is always low and the suggestions are really poor as we end in the \"the boy who boy\" loop very easily (after 1 or 2 words) and cannot create any fable. Training with 5 words inputs on the contrary induces overfitting, we have a very good training accuracy of 99.5% very fast, but in the fable making, pretty fast we come back to the original fable exactly as it was.\n",
    "\n",
    "We can deduce from this that n_input = 3 was probably the optimal paramater for our model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
