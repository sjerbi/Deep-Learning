{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning   </h1>\n",
    "<h1 style=\"text-align:center\"> Lab Session 2 - 3 Hours </h1>\n",
    "<h1 style=\"text-align:center\"> Convolutional Neural Network (CNN) for Handwritten Digits Recognition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Student 1:</b> Sofiene Jerbi \n",
    " \n",
    " \n",
    "The aim of this session is to practice with Convolutional Neural Networks. Answers and experiments should be made by groups of one or two students. Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "\n",
    "Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an pdf document using print as PDF (Ctrl+P). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed by May 29th 2017.\n",
    "\n",
    "Send you pdf file to benoit.huet@eurecom.fr and olfa.ben-ahmed@eurecom.fr using **[DeepLearning_lab2]** as Subject of your email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last Lab Session, you built a Multilayer Perceptron for recognizing hand-written digits from the MNIST data-set. The best achieved accuracy on testing data was about 97%.  Can  you do better than these results using a deep CNN ?\n",
    "In this Lab Session, you will build, train and optimize in TensorFlow one of the early Convolutional Neural Networks:  **LeNet-5** to go to  more than 99% of accuracy. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Data in TensorFlow\n",
    "Run the cell above to load the MNIST data that comes  with TensorFlow. You will use this data in **Section 1** and **Section 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Image Shape: (784,)\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : My First Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before starting with CNN, let's train and test in TensorFlow the example :\n",
    "**y=softmax(Wx+b)** seen in the DeepLearing course last week. \n",
    "\n",
    "This model reaches an accuracy of about 92 %.\n",
    "You will also learn how to launch the tensorBoard https://www.tensorflow.org/get_started/summaries_and_tensorboard to  visualize the computation graph, statistics and learning curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : Read carefully the code in the cell below. Run it to perform training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss= 1.286880516\n",
      "Epoch:  02   =====> Loss= 0.731935138\n",
      "Epoch:  03   =====> Loss= 0.600087293\n",
      "Epoch:  04   =====> Loss= 0.536595575\n",
      "Epoch:  05   =====> Loss= 0.497583865\n",
      "Epoch:  06   =====> Loss= 0.471118211\n",
      "Epoch:  07   =====> Loss= 0.451284961\n",
      "Epoch:  08   =====> Loss= 0.435826128\n",
      "Epoch:  09   =====> Loss= 0.423237018\n",
      "Epoch:  10   =====> Loss= 0.413191887\n",
      "Epoch:  11   =====> Loss= 0.404301464\n",
      "Epoch:  12   =====> Loss= 0.396685983\n",
      "Epoch:  13   =====> Loss= 0.390345196\n",
      "Epoch:  14   =====> Loss= 0.384366291\n",
      "Epoch:  15   =====> Loss= 0.379075426\n",
      "Epoch:  16   =====> Loss= 0.374457767\n",
      "Epoch:  17   =====> Loss= 0.370298153\n",
      "Epoch:  18   =====> Loss= 0.366349395\n",
      "Epoch:  19   =====> Loss= 0.363021322\n",
      "Epoch:  20   =====> Loss= 0.359405888\n",
      "Epoch:  21   =====> Loss= 0.356634224\n",
      "Epoch:  22   =====> Loss= 0.353953617\n",
      "Epoch:  23   =====> Loss= 0.351337360\n",
      "Epoch:  24   =====> Loss= 0.348519755\n",
      "Epoch:  25   =====> Loss= 0.346385461\n",
      "Epoch:  26   =====> Loss= 0.344147922\n",
      "Epoch:  27   =====> Loss= 0.342274432\n",
      "Epoch:  28   =====> Loss= 0.340460419\n",
      "Epoch:  29   =====> Loss= 0.338291559\n",
      "Epoch:  30   =====> Loss= 0.336717768\n",
      "Epoch:  31   =====> Loss= 0.334957561\n",
      "Epoch:  32   =====> Loss= 0.333224993\n",
      "Epoch:  33   =====> Loss= 0.332109854\n",
      "Epoch:  34   =====> Loss= 0.330602574\n",
      "Epoch:  35   =====> Loss= 0.329096714\n",
      "Epoch:  36   =====> Loss= 0.327487422\n",
      "Epoch:  37   =====> Loss= 0.326335563\n",
      "Epoch:  38   =====> Loss= 0.325326298\n",
      "Epoch:  39   =====> Loss= 0.324337772\n",
      "Epoch:  40   =====> Loss= 0.323034117\n",
      "Epoch:  41   =====> Loss= 0.321741484\n",
      "Epoch:  42   =====> Loss= 0.320967023\n",
      "Epoch:  43   =====> Loss= 0.320004324\n",
      "Epoch:  44   =====> Loss= 0.318874341\n",
      "Epoch:  45   =====> Loss= 0.317840827\n",
      "Epoch:  46   =====> Loss= 0.316753561\n",
      "Epoch:  47   =====> Loss= 0.316094517\n",
      "Epoch:  48   =====> Loss= 0.315376098\n",
      "Epoch:  49   =====> Loss= 0.314237537\n",
      "Epoch:  50   =====> Loss= 0.313606572\n",
      "Epoch:  51   =====> Loss= 0.312727793\n",
      "Epoch:  52   =====> Loss= 0.312323817\n",
      "Epoch:  53   =====> Loss= 0.311455045\n",
      "Epoch:  54   =====> Loss= 0.310471629\n",
      "Epoch:  55   =====> Loss= 0.309999790\n",
      "Epoch:  56   =====> Loss= 0.309333011\n",
      "Epoch:  57   =====> Loss= 0.308579979\n",
      "Epoch:  58   =====> Loss= 0.308111550\n",
      "Epoch:  59   =====> Loss= 0.307004378\n",
      "Epoch:  60   =====> Loss= 0.306770415\n",
      "Epoch:  61   =====> Loss= 0.306351947\n",
      "Epoch:  62   =====> Loss= 0.305644147\n",
      "Epoch:  63   =====> Loss= 0.305100559\n",
      "Epoch:  64   =====> Loss= 0.304212520\n",
      "Epoch:  65   =====> Loss= 0.304097393\n",
      "Epoch:  66   =====> Loss= 0.303247741\n",
      "Epoch:  67   =====> Loss= 0.302861805\n",
      "Epoch:  68   =====> Loss= 0.302195784\n",
      "Epoch:  69   =====> Loss= 0.301731664\n",
      "Epoch:  70   =====> Loss= 0.301131797\n",
      "Epoch:  71   =====> Loss= 0.300988282\n",
      "Epoch:  72   =====> Loss= 0.300377518\n",
      "Epoch:  73   =====> Loss= 0.300026636\n",
      "Epoch:  74   =====> Loss= 0.299495686\n",
      "Epoch:  75   =====> Loss= 0.298893366\n",
      "Epoch:  76   =====> Loss= 0.298563705\n",
      "Epoch:  77   =====> Loss= 0.298101432\n",
      "Epoch:  78   =====> Loss= 0.297882539\n",
      "Epoch:  79   =====> Loss= 0.297202785\n",
      "Epoch:  80   =====> Loss= 0.296804053\n",
      "Epoch:  81   =====> Loss= 0.296338048\n",
      "Epoch:  82   =====> Loss= 0.296016949\n",
      "Epoch:  83   =====> Loss= 0.295725127\n",
      "Epoch:  84   =====> Loss= 0.295510158\n",
      "Epoch:  85   =====> Loss= 0.294788248\n",
      "Epoch:  86   =====> Loss= 0.294502434\n",
      "Epoch:  87   =====> Loss= 0.294273335\n",
      "Epoch:  88   =====> Loss= 0.293654532\n",
      "Epoch:  89   =====> Loss= 0.293403858\n",
      "Epoch:  90   =====> Loss= 0.292925803\n",
      "Epoch:  91   =====> Loss= 0.292715266\n",
      "Epoch:  92   =====> Loss= 0.292384265\n",
      "Epoch:  93   =====> Loss= 0.292004094\n",
      "Epoch:  94   =====> Loss= 0.291906941\n",
      "Epoch:  95   =====> Loss= 0.291267684\n",
      "Epoch:  96   =====> Loss= 0.291257095\n",
      "Epoch:  97   =====> Loss= 0.290811745\n",
      "Epoch:  98   =====> Loss= 0.290831424\n",
      "Epoch:  99   =====> Loss= 0.290203090\n",
      "Epoch:  100   =====> Loss= 0.290146119\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9202\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#STEP 1\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([10]), name='Bias')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "#STEP 2 \n",
    "\n",
    "\n",
    "# Launch the graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2  </b>: Using Tensorboard, we can  now visualize the created graph, giving you an overview of your architecture and how all of the major components  are connected. You can also see and analyse the learning curves. \n",
    "\n",
    "To launch tensorBoard: \n",
    "- Go to the **TP2** folder, \n",
    "- Open a Terminal and run the command line **\"tensorboard --logdir=log_files/\"**, it will generate an http link ,ex http://666.6.6.6:6006,\n",
    "- Copy this  link into your web browser \n",
    "\n",
    "\n",
    "Enjoy It !! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : The 99% MNIST Challenge !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : LeNet5 implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One you are now familar with **tensorFlow** and **tensorBoard**, you are in this section to build, train and test the baseline [LeNet-5](http://yann.lecun.com/exdb/lenet/)  model for the MNIST digits recognition problem.  \n",
    "\n",
    "In more advanced step you will make some optimizations to get more than 99% of accuracy. The best model can get to over 99.7% accuracy! \n",
    "\n",
    "For more information, have a look at this list of results : http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"lenet.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 1: Lenet 5 </span></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "--------------------------\n",
    "**Layer 1: Convolutional.** The output shape should be 28x28x6 **Activation.** sigmoid **Pooling.** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2: Convolutional.** The output shape should be 10x10x16. **Activation.** sigmoid **Pooling.** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.  You may need to use **flatten*  from tensorflow.contrib.layers import flatten\n",
    "\n",
    "**Layer 3: Fully Connected.** This should have 120 outputs. **Activation.** sigmoid\n",
    "\n",
    "**Layer 4: Fully Connected.** This should have 84 outputs. **Activation.** sigmoid\n",
    "\n",
    "**Layer 5: Fully Connected.** This should have 10 outputs. **Activation.** softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.1 </b>  Implement the Neural Network architecture described above.\n",
    "For that, your will use classes and functions from  https://www.tensorflow.org/api_docs/python/tf/nn. \n",
    "\n",
    "We give you some helper functions for weigths and bias initilization. Also you can refer to section 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#Helper functions  for weigths and bias initilization \n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial, name='Weights')\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial, name='Bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet5_Model(data):\n",
    "    data = tf.reshape(data, [-1, 28, 28, 1])\n",
    "    # Set layer1 weights\n",
    "    W_layer1 = weight_variable([5, 5, 1, 6])\n",
    "    b_layer1 = bias_variable([6])\n",
    "    \n",
    "    # Set layer2 weights\n",
    "    W_layer2 = weight_variable([5, 5, 6, 16])\n",
    "    b_layer2 = bias_variable([16])\n",
    "    \n",
    "    # Set layer3 weights\n",
    "    W_layer3 = weight_variable([400,120])\n",
    "    b_layer3 = bias_variable([120])\n",
    "    \n",
    "    # Set layer4 weights\n",
    "    W_layer4 = weight_variable([120,84])\n",
    "    b_layer4 = bias_variable([84])\n",
    "    \n",
    "    # Set layer5 weights\n",
    "    W_layer5 = weight_variable([84,10])\n",
    "    b_layer5 = bias_variable([10])\n",
    "\n",
    "    # Construct model\n",
    "    layer1 = tf.nn.sigmoid(tf.nn.conv2d(data, W_layer1, [1, 1, 1, 1], padding='SAME') + b_layer1)\n",
    "    pooling1 = tf.nn.max_pool(layer1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    layer2 = tf.nn.sigmoid(tf.nn.conv2d(pooling1, W_layer2, [1, 1, 1, 1], padding='VALID') + b_layer2)\n",
    "    pooling2 = tf.nn.max_pool(layer2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    flattened = tf.contrib.layers.flatten(pooling2)\n",
    "    layer3 = tf.nn.sigmoid(tf.matmul(flattened, W_layer3) + b_layer3)\n",
    "    layer4 = tf.nn.sigmoid(tf.matmul(layer3, W_layer4) + b_layer4)\n",
    "    \n",
    "    with tf.name_scope('Model'):\n",
    "        # Model\n",
    "        model = tf.nn.softmax(tf.matmul(layer4, W_layer5) + b_layer5) # Softmax\n",
    "    return model\n",
    "\n",
    "model = LeNet5_Model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.2. </b>  Calculate the number of parameters of this model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>- For convolutional layers:</b><br>\n",
    "    weights = filter_height \\* filter_width \\* input_channels \\* output_channels<br>\n",
    "    biases = output_channels<br>\n",
    "<b>- For pooling:</b><br>\n",
    "    pooling = 0 because no weights or biases<br>\n",
    "<b>- For fully connected layers:</b><br>\n",
    "    weights = input * output<br>\n",
    "    biases = output<br>\n",
    "\n",
    "<b>#layer1</b><br>\n",
    "weights = 5 * 5 * 1 * 6 = 150<br>\n",
    "biases = 6<br>\n",
    "pooling = 0<br>\n",
    "\n",
    "<b>#layer2</b><br>\n",
    "weights = 5 * 5 * 6 * 16 = 900<br>\n",
    "biases = 16<br>\n",
    "pooling = 0<br>\n",
    "\n",
    "<b>#layer3</b><br>\n",
    "weights = 400 * 120 = 48 000<br>\n",
    "biases = 120<br>\n",
    "\n",
    "<b>#layer4</b><br>\n",
    "weights = 120 * 84 = 10 080<br>\n",
    "biases = 84<br>\n",
    "\n",
    "<b>#layer5</b><br>\n",
    "weights = 84 * 10 = 840<br>\n",
    "biases = 10<br>\n",
    "\n",
    "<b>Total number of parameters:</b> 60 206"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.3. </b>  Start the training with the parameters cited below:\n",
    "\n",
    "     Learning rate =0.1\n",
    "     Loss Fucntion : Cross entropy\n",
    "     Optimisateur: SGD\n",
    "     Number of training iterations= 10000\n",
    "     The batch size =128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters \n",
    "learning_rate = 0.1\n",
    "training_epochs = 1000\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "saving_path = 'models/'\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(model), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, y):\n",
    "    acc = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    accuracy = evaluate(model, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.5. </b>  Implement training pipeline and run the training data through it to train the model.\n",
    "\n",
    "- Before each epoch, shuffle the training set. \n",
    "- Print the loss per mini batch and the training/validation accuracy per epoch. (Display results every 100 epochs)\n",
    "- Save the model after training\n",
    "- Print after training the final testing accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(learning_rate, training_epochs, batch_size, cost, optimizer):\n",
    "    # Initializing the session \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        # op to write logs to Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        # Add ops to save and restore all the variables.\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        print (\"Start Training!\")\n",
    "        \n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "                # Run optimization op (backprop), cost op (to get loss value)\n",
    "                # and summary nodes\n",
    "                _, c, summary = sess.run([optimizer, cost, merged_summary_op], feed_dict={x: batch_xs, y: batch_ys})\n",
    "                # Write logs at every iteration\n",
    "                summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "            # Display logs per epoch step\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost), \"  Validation Accuracy=\", \"{:.9f}\".format(accuracy.eval(feed_dict={x: mnist.validation.images, y: mnist.validation.labels})), \"  Test Accuracy=\", \"{:.9f}\".format(accuracy.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels})))\n",
    "\n",
    "        print (\"Training Finished!\")\n",
    "        save_path = saver.save(sess, saving_path+\"/model\"+str(int(time.time())))\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "        print(\"Final accuracy:\", accuracy.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "Epoch:  100   =====> Loss= 0.051045185   Accuracy= 0.981999993\n",
      "Epoch:  200   =====> Loss= 0.017796859   Accuracy= 0.989199996\n",
      "Epoch:  300   =====> Loss= 0.007578073   Accuracy= 0.988600016\n",
      "Epoch:  400   =====> Loss= 0.004146392   Accuracy= 0.988600016\n",
      "Epoch:  500   =====> Loss= 0.002333913   Accuracy= 0.988399982\n",
      "Epoch:  600   =====> Loss= 0.001403212   Accuracy= 0.988900006\n",
      "Epoch:  700   =====> Loss= 0.000972731   Accuracy= 0.988699973\n",
      "Epoch:  800   =====> Loss= 0.000746378   Accuracy= 0.988900006\n",
      "Epoch:  900   =====> Loss= 0.000604468   Accuracy= 0.988799989\n",
      "Epoch:  1000   =====> Loss= 0.000467924   Accuracy= 0.989000022\n",
      "Training Finished!\n",
      "Model saved in file: models//model1495575909\n",
      "Final accuracy: 0.989\n"
     ]
    }
   ],
   "source": [
    "#Launch training\n",
    "train(learning_rate, training_epochs, batch_size, cost, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['  def next_batch(self, batch_size, fake_data=False):\\n',\n",
       "  '    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\\n',\n",
       "  '    if fake_data:\\n',\n",
       "  '      fake_image = [1] * 784\\n',\n",
       "  '      if self.one_hot:\\n',\n",
       "  '        fake_label = [1] + [0] * 9\\n',\n",
       "  '      else:\\n',\n",
       "  '        fake_label = 0\\n',\n",
       "  '      return [fake_image for _ in xrange(batch_size)], [\\n',\n",
       "  '          fake_label for _ in xrange(batch_size)\\n',\n",
       "  '      ]\\n',\n",
       "  '    start = self._index_in_epoch\\n',\n",
       "  '    self._index_in_epoch += batch_size\\n',\n",
       "  '    if self._index_in_epoch > self._num_examples:\\n',\n",
       "  '      # Finished epoch\\n',\n",
       "  '      self._epochs_completed += 1\\n',\n",
       "  '      # Shuffle the data\\n',\n",
       "  '      perm = numpy.arange(self._num_examples)\\n',\n",
       "  '      numpy.random.shuffle(perm)\\n',\n",
       "  '      self._images = self._images[perm]\\n',\n",
       "  '      self._labels = self._labels[perm]\\n',\n",
       "  '      # Start next epoch\\n',\n",
       "  '      start = 0\\n',\n",
       "  '      self._index_in_epoch = batch_size\\n',\n",
       "  '      assert batch_size <= self._num_examples\\n',\n",
       "  '    end = self._index_in_epoch\\n',\n",
       "  '    return self._images[start:end], self._labels[start:end]\\n'],\n",
       " 160)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "inspect.getsourcelines(tf.contrib.learn.datasets.mnist.DataSet.next_batch) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.6 </b> : Use tensorBoard to visualise and save the LeNet5 Graph and all learning curves. \n",
    "Save all obtained figures in the folder **\"TP2/MNIST_99_Challenge_Figures\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"MNIST_99_Challenge_Figures/Accuracy_2.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<img src=\"MNIST_99_Challenge_Figures/Loss_2.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<img src=\"MNIST_99_Challenge_Figures/Graph_2.png\",align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We train our model for 1000 epochs with a batch size of 128 and we successfully get a final test accuracy of 98.9%. What we also see is that we got that result since the epoch 200, meaning that these many epochs are unecessary, and because they take so much time, we will rather train our models with only 100 epochs from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2 </b> : LeNET 5 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b> Question 2.2.1 </b>  Change the sigmoid function with a Relu :\n",
    "\n",
    "- Retrain your network with SGD and AdamOptimizer and then fill the table above  :\n",
    "\n",
    "\n",
    "| Optimizer            |  Gradient Descent         |AdamOptimizer |\n",
    "| -------------        |: -------------: | ---------:   \n",
    "| Validation Accuracy  |   0.990400016   |  0.097599998  |      \n",
    "| Testing Accuracy     | 0.990599990          |  0.103200004  |       \n",
    "| Training Time        |  1.03min/epoch         |   1.21min/epoch     |  |  \n",
    "\n",
    "\n",
    "- Try with different learning rates for each Optimizer (0.0001 and 0.001 ) and different Batch sizes (50 and 128) for 20000 Epochs. \n",
    "\n",
    "- For each optimizer, plot (on the same curve) the **testing accuracies** function to **(learning rate, batch size)** \n",
    "\n",
    "\n",
    "\n",
    "- Did you reach the 99% accuracy ? What are the optimal parametres that gave you the best results? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Training with SGD***\n",
      "Start Training!\n",
      "Epoch:  10   =====> Loss= 0.023872270   Validation Accuracy= 0.988799989   Test Accuracy= 0.987999976\n",
      "Epoch:  20   =====> Loss= 0.007036820   Validation Accuracy= 0.990800023   Test Accuracy= 0.989000022\n",
      "Epoch:  30   =====> Loss= 0.001859246   Validation Accuracy= 0.990599990   Test Accuracy= 0.990300000\n",
      "Epoch:  40   =====> Loss= 0.000462042   Validation Accuracy= 0.990599990   Test Accuracy= 0.990300000\n",
      "Epoch:  50   =====> Loss= 0.000115001   Validation Accuracy= 0.990400016   Test Accuracy= 0.990599990\n",
      "Epoch:  60   =====> Loss= 0.000081626   Validation Accuracy= 0.990199983   Test Accuracy= 0.990700006\n",
      "Epoch:  70   =====> Loss= 0.000061326   Validation Accuracy= 0.990199983   Test Accuracy= 0.990700006\n",
      "Epoch:  80   =====> Loss= 0.000049109   Validation Accuracy= 0.989799976   Test Accuracy= 0.990400016\n",
      "Epoch:  90   =====> Loss= 0.000041476   Validation Accuracy= 0.990000010   Test Accuracy= 0.990499973\n",
      "Epoch:  100   =====> Loss= 0.000035297   Validation Accuracy= 0.989600003   Test Accuracy= 0.990100026\n",
      "Training Finished!\n",
      "Model saved in file: models//model1496069984\n",
      "Final accuracy: 0.9901\n",
      "Time taken:  6165.006519079208\n",
      "***Training with Adam***\n",
      "Start Training!\n",
      "Epoch:  10   =====> Loss= 16.607530040   Validation Accuracy= 0.097599998   Test Accuracy= 0.103200004\n",
      "Epoch:  20   =====> Loss= 16.594782669   Validation Accuracy= 0.097599998   Test Accuracy= 0.103200004\n",
      "Epoch:  30   =====> Loss= 16.580357972   Validation Accuracy= 0.097599998   Test Accuracy= 0.103200004\n",
      "Epoch:  40   =====> Loss= 16.596459964   Validation Accuracy= 0.097599998   Test Accuracy= 0.103200004\n",
      "Epoch:  50   =====> Loss= 16.577003392   Validation Accuracy= 0.097599998   Test Accuracy= 0.103200004\n",
      "Epoch:  60   =====> Loss= 16.594447196   Validation Accuracy= 0.097599998   Test Accuracy= 0.103200004\n",
      "Epoch:  70   =====> Loss= 16.578009790   Validation Accuracy= 0.097599998   Test Accuracy= 0.103200004\n",
      "Epoch:  80   =====> Loss= 16.610549164   Validation Accuracy= 0.097599998   Test Accuracy= 0.103200004\n",
      "Epoch:  90   =====> Loss= 16.585054380   Validation Accuracy= 0.097599998   Test Accuracy= 0.103200004\n",
      "Epoch:  100   =====> Loss= 16.596459947   Validation Accuracy= 0.097599998   Test Accuracy= 0.103200004\n",
      "Training Finished!\n",
      "Model saved in file: models//model1496077225\n",
      "Final accuracy: 0.1032\n",
      "Time taken:  7239.271738767624\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "import time\n",
    "\n",
    "def LeNet5_Model_Relu(data):\n",
    "    data = tf.reshape(data, [-1, 28, 28, 1])\n",
    "    # Set layer1 weights\n",
    "    W_layer1 = weight_variable([5, 5, 1, 6])\n",
    "    b_layer1 = bias_variable([6])\n",
    "    \n",
    "    # Set layer2 weights\n",
    "    W_layer2 = weight_variable([5, 5, 6, 16])\n",
    "    b_layer2 = bias_variable([16])\n",
    "    \n",
    "    # Set layer3 weights\n",
    "    W_layer3 = weight_variable([400,120])\n",
    "    b_layer3 = bias_variable([120])\n",
    "    \n",
    "    # Set layer4 weights\n",
    "    W_layer4 = weight_variable([120,84])\n",
    "    b_layer4 = bias_variable([84])\n",
    "    \n",
    "    # Set layer5 weights\n",
    "    W_layer5 = weight_variable([84,10])\n",
    "    b_layer5 = bias_variable([10])\n",
    "\n",
    "    # Construct model\n",
    "    layer1 = tf.nn.relu(tf.nn.conv2d(data, W_layer1, [1, 1, 1, 1], padding='SAME') + b_layer1)\n",
    "    pooling1 = tf.nn.max_pool(layer1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    layer2 = tf.nn.relu(tf.nn.conv2d(pooling1, W_layer2, [1, 1, 1, 1], padding='VALID') + b_layer2)\n",
    "    pooling2 = tf.nn.max_pool(layer2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    flattened = tf.contrib.layers.flatten(pooling2)\n",
    "    layer3 = tf.nn.relu(tf.matmul(flattened, W_layer3) + b_layer3)\n",
    "    layer4 = tf.nn.relu(tf.matmul(layer3, W_layer4) + b_layer4)\n",
    "    \n",
    "    with tf.name_scope('Model'):\n",
    "        # Model\n",
    "        model = tf.nn.softmax(tf.matmul(layer4, W_layer5) + b_layer5) # Softmax\n",
    "    return model\n",
    "\n",
    "model_relu = LeNet5_Model_Relu(x)\n",
    "\n",
    "# Training parameters \n",
    "learning_rate = 0.1\n",
    "training_epochs = 100\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "saving_path = 'models/'\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(model_relu+1e-8), reduction_indices=1))\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    accuracy = evaluate(model_relu, y)\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()    \n",
    "\n",
    "print(\"***Training with SGD***\")\n",
    "t0 = time.time()\n",
    "train(learning_rate, training_epochs, batch_size, cost, optimizer)\n",
    "print(\"Time taken: \",time.time()-t0)\n",
    "\n",
    "model_relu = LeNet5_Model_Relu(x)\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(model_relu+1e-8), reduction_indices=1))\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    accuracy = evaluate(model_relu, y)\n",
    "with tf.name_scope('Adam'):\n",
    "    # Adam Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()    \n",
    "\n",
    "print(\"***Training with Adam***\")\n",
    "t0 = time.time()\n",
    "train(learning_rate, training_epochs, batch_size, cost, optimizer)\n",
    "print(\"Time taken: \",time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Training with SGD 0.0001 50***\n",
      "Start Training!\n",
      "Epoch:  10   =====> Loss= 2.183650568   Validation Accuracy= 0.403400004   Test Accuracy= 0.399899989\n",
      "Epoch:  20   =====> Loss= 1.381050881   Validation Accuracy= 0.751600027   Test Accuracy= 0.757000029\n",
      "Epoch:  30   =====> Loss= 0.542827971   Validation Accuracy= 0.869400024   Test Accuracy= 0.869099975\n",
      "Epoch:  40   =====> Loss= 0.399499516   Validation Accuracy= 0.892199993   Test Accuracy= 0.894599974\n",
      "Epoch:  50   =====> Loss= 0.342217829   Validation Accuracy= 0.906400025   Test Accuracy= 0.903599977\n",
      "Epoch:  60   =====> Loss= 0.305464724   Validation Accuracy= 0.913999975   Test Accuracy= 0.912999988\n",
      "Epoch:  70   =====> Loss= 0.280051400   Validation Accuracy= 0.921800017   Test Accuracy= 0.920799971\n",
      "Epoch:  80   =====> Loss= 0.263080859   Validation Accuracy= 0.927399993   Test Accuracy= 0.927399993\n",
      "Epoch:  90   =====> Loss= 0.242744129   Validation Accuracy= 0.932200015   Test Accuracy= 0.932900012\n",
      "Epoch:  100   =====> Loss= 0.229166657   Validation Accuracy= 0.936200023   Test Accuracy= 0.937200010\n",
      "Training Finished!\n",
      "Model saved in file: models//model1496088882\n",
      "Final accuracy: 0.9372\n",
      "Time taken:  8206.934541463852\n",
      "***Training with Adam 0.0001 50***\n",
      "Start Training!\n",
      "Epoch:  10   =====> Loss= 0.055596082   Validation Accuracy= 0.982999980   Test Accuracy= 0.984499991\n",
      "Epoch:  20   =====> Loss= 0.029967740   Validation Accuracy= 0.987600029   Test Accuracy= 0.987999976\n",
      "Epoch:  30   =====> Loss= 0.017176215   Validation Accuracy= 0.989199996   Test Accuracy= 0.988900006\n",
      "Epoch:  40   =====> Loss= 0.010373591   Validation Accuracy= 0.987399995   Test Accuracy= 0.989499986\n",
      "Epoch:  50   =====> Loss= 0.006242598   Validation Accuracy= 0.989799976   Test Accuracy= 0.989600003\n",
      "Epoch:  60   =====> Loss= 0.003290361   Validation Accuracy= 0.990000010   Test Accuracy= 0.990199983\n",
      "Epoch:  70   =====> Loss= 0.001932718   Validation Accuracy= 0.990000010   Test Accuracy= 0.989899993\n",
      "Epoch:  80   =====> Loss= 0.003126191   Validation Accuracy= 0.989799976   Test Accuracy= 0.989199996\n",
      "Epoch:  90   =====> Loss= 0.001873693   Validation Accuracy= 0.991999984   Test Accuracy= 0.989700019\n",
      "Epoch:  100   =====> Loss= 0.003075307   Validation Accuracy= 0.990999997   Test Accuracy= 0.990400016\n",
      "Training Finished!\n",
      "Model saved in file: models//model1496098599\n",
      "Final accuracy: 0.9904\n",
      "Time taken:  9717.333002567291\n",
      "***Training with SGD 0.0001 128***\n",
      "Start Training!\n",
      "Epoch:  10   =====> Loss= 2.267960256   Validation Accuracy= 0.190400004   Test Accuracy= 0.186199993\n",
      "Epoch:  20   =====> Loss= 2.189814518   Validation Accuracy= 0.348399997   Test Accuracy= 0.348300010\n",
      "Epoch:  30   =====> Loss= 2.030172196   Validation Accuracy= 0.421000004   Test Accuracy= 0.436899990\n",
      "Epoch:  40   =====> Loss= 1.665928499   Validation Accuracy= 0.552600026   Test Accuracy= 0.569299996\n",
      "Epoch:  50   =====> Loss= 1.150159707   Validation Accuracy= 0.707799971   Test Accuracy= 0.721000016\n",
      "Epoch:  60   =====> Loss= 0.790583456   Validation Accuracy= 0.787400007   Test Accuracy= 0.799600005\n",
      "Epoch:  70   =====> Loss= 0.611063011   Validation Accuracy= 0.837199986   Test Accuracy= 0.841499984\n",
      "Epoch:  80   =====> Loss= 0.510257417   Validation Accuracy= 0.863399982   Test Accuracy= 0.864600003\n",
      "Epoch:  90   =====> Loss= 0.454865207   Validation Accuracy= 0.879000008   Test Accuracy= 0.882399976\n",
      "Epoch:  100   =====> Loss= 0.412661143   Validation Accuracy= 0.889199972   Test Accuracy= 0.890999973\n",
      "Training Finished!\n",
      "Model saved in file: models//model1496108736\n",
      "Final accuracy: 0.891\n",
      "Time taken:  10137.226004362106\n",
      "***Training with Adam 0.0001 128***\n",
      "Start Training!\n",
      "Epoch:  10   =====> Loss= 0.100005356   Validation Accuracy= 0.973399997   Test Accuracy= 0.972700000\n",
      "Epoch:  20   =====> Loss= 0.057597964   Validation Accuracy= 0.984000027   Test Accuracy= 0.982500017\n",
      "Epoch:  30   =====> Loss= 0.039821176   Validation Accuracy= 0.986800015   Test Accuracy= 0.984700024\n",
      "Epoch:  40   =====> Loss= 0.029607774   Validation Accuracy= 0.987200022   Test Accuracy= 0.985899985\n",
      "Epoch:  50   =====> Loss= 0.021551693   Validation Accuracy= 0.989199996   Test Accuracy= 0.987900019\n",
      "Epoch:  60   =====> Loss= 0.015574807   Validation Accuracy= 0.990999997   Test Accuracy= 0.988099992\n",
      "Epoch:  70   =====> Loss= 0.011117515   Validation Accuracy= 0.989600003   Test Accuracy= 0.988099992\n",
      "Epoch:  80   =====> Loss= 0.008883428   Validation Accuracy= 0.987800002   Test Accuracy= 0.988200009\n",
      "Epoch:  90   =====> Loss= 0.005667544   Validation Accuracy= 0.990199983   Test Accuracy= 0.988200009\n",
      "Epoch:  100   =====> Loss= 0.004586646   Validation Accuracy= 0.988799989   Test Accuracy= 0.988300025\n",
      "Training Finished!\n",
      "Model saved in file: models//model1496119874\n",
      "Final accuracy: 0.9883\n",
      "Time taken:  11141.78365445137\n",
      "***Training with SGD 0.001 50***\n",
      "Start Training!\n",
      "Epoch:  10   =====> Loss= 0.247223169   Validation Accuracy= 0.933200002   Test Accuracy= 0.932099998\n",
      "Epoch:  20   =====> Loss= 0.156832987   Validation Accuracy= 0.958000004   Test Accuracy= 0.959200025\n",
      "Epoch:  30   =====> Loss= 0.115406103   Validation Accuracy= 0.969799995   Test Accuracy= 0.967999995\n",
      "Epoch:  40   =====> Loss= 0.093579785   Validation Accuracy= 0.971800029   Test Accuracy= 0.972000003\n",
      "Epoch:  50   =====> Loss= 0.077916296   Validation Accuracy= 0.975600004   Test Accuracy= 0.976499975\n",
      "Epoch:  60   =====> Loss= 0.069986910   Validation Accuracy= 0.978799999   Test Accuracy= 0.975199997\n",
      "Epoch:  70   =====> Loss= 0.062621944   Validation Accuracy= 0.979200006   Test Accuracy= 0.978600025\n",
      "Epoch:  80   =====> Loss= 0.056059621   Validation Accuracy= 0.980199993   Test Accuracy= 0.981199980\n",
      "Epoch:  90   =====> Loss= 0.052101222   Validation Accuracy= 0.982200027   Test Accuracy= 0.981299996\n",
      "Epoch:  100   =====> Loss= 0.047704714   Validation Accuracy= 0.983600020   Test Accuracy= 0.981299996\n",
      "Training Finished!\n",
      "Model saved in file: models//model1496133271\n",
      "Final accuracy: 0.9813\n",
      "Time taken:  13395.553267717361\n",
      "***Training with Adam 0.001 50***\n",
      "Start Training!\n",
      "Epoch:  10   =====> Loss= 0.014787138   Validation Accuracy= 0.989600003   Test Accuracy= 0.988499999\n",
      "Epoch:  20   =====> Loss= 0.007054455   Validation Accuracy= 0.991999984   Test Accuracy= 0.989700019\n",
      "Epoch:  30   =====> Loss= 0.005388132   Validation Accuracy= 0.990000010   Test Accuracy= 0.990700006\n",
      "Epoch:  40   =====> Loss= 0.001798739   Validation Accuracy= 0.989000022   Test Accuracy= 0.987800002\n",
      "Epoch:  50   =====> Loss= 0.004623877   Validation Accuracy= 0.991199970   Test Accuracy= 0.989400029\n",
      "Epoch:  60   =====> Loss= 0.004163758   Validation Accuracy= 0.991599977   Test Accuracy= 0.989499986\n",
      "Epoch:  70   =====> Loss= 0.003301584   Validation Accuracy= 0.991199970   Test Accuracy= 0.990199983\n",
      "Epoch:  80   =====> Loss= 0.004037532   Validation Accuracy= 0.991199970   Test Accuracy= 0.989099979\n",
      "Epoch:  90   =====> Loss= 0.003264156   Validation Accuracy= 0.990400016   Test Accuracy= 0.987299979\n",
      "Epoch:  100   =====> Loss= 0.004765530   Validation Accuracy= 0.991199970   Test Accuracy= 0.990000010\n",
      "Training Finished!\n",
      "Model saved in file: models//model1496145918\n",
      "Final accuracy: 0.99\n",
      "Time taken:  12647.262574911118\n",
      "***Training with SGD 0.001 128***\n",
      "Start Training!\n",
      "Epoch:  10   =====> Loss= 0.437160588   Validation Accuracy= 0.883599997   Test Accuracy= 0.891600013\n",
      "Epoch:  20   =====> Loss= 0.271313700   Validation Accuracy= 0.925800025   Test Accuracy= 0.925999999\n",
      "Epoch:  30   =====> Loss= 0.209158655   Validation Accuracy= 0.943400025   Test Accuracy= 0.942499995\n",
      "Epoch:  40   =====> Loss= 0.171743240   Validation Accuracy= 0.956799984   Test Accuracy= 0.953599989\n",
      "Epoch:  50   =====> Loss= 0.142268099   Validation Accuracy= 0.963800013   Test Accuracy= 0.961600006\n",
      "Epoch:  60   =====> Loss= 0.121133098   Validation Accuracy= 0.967000008   Test Accuracy= 0.966199994\n",
      "Epoch:  70   =====> Loss= 0.107532560   Validation Accuracy= 0.969200015   Test Accuracy= 0.968599975\n",
      "Epoch:  80   =====> Loss= 0.096601427   Validation Accuracy= 0.972199976   Test Accuracy= 0.972400010\n",
      "Epoch:  90   =====> Loss= 0.089484734   Validation Accuracy= 0.975000024   Test Accuracy= 0.973900020\n",
      "Epoch:  100   =====> Loss= 0.082230644   Validation Accuracy= 0.975799978   Test Accuracy= 0.976300001\n",
      "Training Finished!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: models//model1496159956\n",
      "Final accuracy: 0.9763\n",
      "Time taken:  14039.092363834381\n",
      "***Training with Adam 0.001 128***\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.0001, 0.001]\n",
    "batch_sizes = [50, 128]\n",
    "training_epochs = 100\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "saving_path = 'models/'\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        model_relu = LeNet5_Model_Relu(x)\n",
    "        with tf.name_scope('Loss'):\n",
    "            # Minimize error using cross entropy\n",
    "            cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(model_relu+1e-8), reduction_indices=1))\n",
    "        with tf.name_scope('Accuracy'):\n",
    "            # Accuracy\n",
    "            accuracy = evaluate(model_relu, y)\n",
    "        with tf.name_scope('SGD'):\n",
    "            # Gradient Descent\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "        # Initializing the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        # Create a summary to monitor cost tensor\n",
    "        tf.summary.scalar(\"Loss\", cost)\n",
    "        # Create a summary to monitor accuracy tensor\n",
    "        tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "        # Merge all summaries into a single op\n",
    "        merged_summary_op = tf.summary.merge_all()    \n",
    "\n",
    "        print(\"***Training with SGD \"+str(learning_rate)+\" \"+str(batch_size)+\"***\")\n",
    "        t0 = time.time()\n",
    "        train(learning_rate, training_epochs, batch_size, cost, optimizer)\n",
    "        print(\"Time taken: \",time.time()-t0)\n",
    "        \n",
    "        model_relu = LeNet5_Model_Relu(x)\n",
    "        with tf.name_scope('Loss'):\n",
    "            # Minimize error using cross entropy\n",
    "            cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(model_relu+1e-8), reduction_indices=1))\n",
    "        with tf.name_scope('Accuracy'):\n",
    "            # Accuracy\n",
    "            accuracy = evaluate(model_relu, y)\n",
    "        with tf.name_scope('Adam'):\n",
    "            # Adam Optimizer\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "        # Initializing the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        # Create a summary to monitor cost tensor\n",
    "        tf.summary.scalar(\"Loss\", cost)\n",
    "        # Create a summary to monitor accuracy tensor\n",
    "        tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "        # Merge all summaries into a single op\n",
    "        merged_summary_op = tf.summary.merge_all()    \n",
    "\n",
    "        print(\"***Training with Adam \"+str(learning_rate)+\" \"+str(batch_size)+\"***\")\n",
    "        t0 = time.time()\n",
    "        train(learning_rate, training_epochs, batch_size, cost, optimizer)\n",
    "        print(\"Time taken: \",time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAawAAAEXCAYAAADxxXAaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x32305a2160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "testing_accuracies_SGD = [0.937200010, 0.890999973, 0.981299996, 0.976300001]\n",
    "testing_accuracies_Adam = [0.990400016, 0.988300025, 0.990700006, 0.990800023]\n",
    "params = list(itertools.product(learning_rates, batch_sizes))\n",
    "idx = list(range(len(params)))\n",
    "\n",
    "p1=plt.plot(idx,testing_accuracies_SGD)\n",
    "p2=plt.plot(idx,testing_accuracies_Adam)\n",
    "plt.xticks(idx, params)\n",
    "plt.legend([\"SGD\", \"Adam\"])\n",
    "plt.xlabel(\"(learning_rate, batch_size)\")\n",
    "plt.ylabel(\"Testing accuracy\")\n",
    "plt.title(\"Testing accuracy with different training parameters and optimizers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally reach our 99% of accuracy with several sets of parameters. The one we got the best result with is:\n",
    "- Learning rate: 0.001\n",
    "- Batch size: 128\n",
    "- Optimizer: Adam\n",
    "with 99.08% of test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.2.2 </b>  What about applying a dropout layer on the Fully conntected layer and then retraining the model with the best Optimizer and parameters(Learning rate and Batch size) obtained in *Question 2.2.1*  ? (probability to keep units=0.75). For this stage ensure that the keep prob is set to 1.0 to evaluate the \n",
    "performance of the network including all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Training with optimal paramiters***\n",
      "Start Training!\n",
      "Epoch:  01   =====> Loss= 0.469975067   Accuracy= 0.956900001\n",
      "Epoch:  02   =====> Loss= 0.124659879   Accuracy= 0.974500000\n",
      "Epoch:  03   =====> Loss= 0.086438573   Accuracy= 0.975799978\n",
      "Epoch:  04   =====> Loss= 0.071557593   Accuracy= 0.981700003\n",
      "Epoch:  05   =====> Loss= 0.059307722   Accuracy= 0.982599974\n",
      "Epoch:  06   =====> Loss= 0.052442966   Accuracy= 0.985700011\n",
      "Epoch:  07   =====> Loss= 0.048454356   Accuracy= 0.984399974\n",
      "Epoch:  08   =====> Loss= 0.040807218   Accuracy= 0.985899985\n",
      "Epoch:  09   =====> Loss= 0.035818005   Accuracy= 0.987500012\n",
      "Epoch:  10   =====> Loss= 0.034113269   Accuracy= 0.986500025\n",
      "Epoch:  11   =====> Loss= 0.032113095   Accuracy= 0.985899985\n",
      "Epoch:  12   =====> Loss= 0.029398655   Accuracy= 0.986500025\n",
      "Epoch:  13   =====> Loss= 0.027292354   Accuracy= 0.987699986\n",
      "Epoch:  14   =====> Loss= 0.027313543   Accuracy= 0.986899972\n",
      "Epoch:  15   =====> Loss= 0.023507383   Accuracy= 0.988900006\n",
      "Epoch:  16   =====> Loss= 0.022867691   Accuracy= 0.986599982\n",
      "Epoch:  17   =====> Loss= 0.020869639   Accuracy= 0.988099992\n",
      "Epoch:  18   =====> Loss= 0.018762498   Accuracy= 0.986699998\n",
      "Epoch:  19   =====> Loss= 0.019554815   Accuracy= 0.988699973\n",
      "Epoch:  20   =====> Loss= 0.019994466   Accuracy= 0.989199996\n",
      "Epoch:  21   =====> Loss= 0.017179678   Accuracy= 0.990800023\n",
      "Epoch:  22   =====> Loss= 0.014795903   Accuracy= 0.988200009\n",
      "Epoch:  23   =====> Loss= 0.015618181   Accuracy= 0.988399982\n",
      "Epoch:  24   =====> Loss= 0.016106847   Accuracy= 0.989600003\n",
      "Epoch:  25   =====> Loss= 0.014001186   Accuracy= 0.989700019\n",
      "Epoch:  26   =====> Loss= 0.015313651   Accuracy= 0.990300000\n",
      "Epoch:  27   =====> Loss= 0.012131293   Accuracy= 0.989700019\n",
      "Epoch:  28   =====> Loss= 0.012936561   Accuracy= 0.987699986\n",
      "Epoch:  29   =====> Loss= 0.014276776   Accuracy= 0.986500025\n",
      "Epoch:  30   =====> Loss= 0.012777669   Accuracy= 0.988300025\n",
      "Epoch:  31   =====> Loss= 0.012943432   Accuracy= 0.990100026\n",
      "Epoch:  32   =====> Loss= 0.010959325   Accuracy= 0.990599990\n",
      "Epoch:  33   =====> Loss= 0.011657129   Accuracy= 0.989499986\n",
      "Epoch:  34   =====> Loss= 0.010827916   Accuracy= 0.988699973\n",
      "Epoch:  35   =====> Loss= 0.011319958   Accuracy= 0.989400029\n",
      "Epoch:  36   =====> Loss= 0.011001677   Accuracy= 0.988900006\n",
      "Epoch:  37   =====> Loss= 0.011529827   Accuracy= 0.988900006\n",
      "Epoch:  38   =====> Loss= 0.008749400   Accuracy= 0.989199996\n",
      "Epoch:  39   =====> Loss= 0.011033263   Accuracy= 0.989199996\n",
      "Epoch:  40   =====> Loss= 0.009633611   Accuracy= 0.987900019\n",
      "Epoch:  41   =====> Loss= 0.009153600   Accuracy= 0.989700019\n",
      "Epoch:  42   =====> Loss= 0.009808642   Accuracy= 0.990100026\n",
      "Epoch:  43   =====> Loss= 0.008143535   Accuracy= 0.989000022\n",
      "Epoch:  44   =====> Loss= 0.007101719   Accuracy= 0.989600003\n",
      "Epoch:  45   =====> Loss= 0.009696418   Accuracy= 0.989300013\n",
      "Epoch:  46   =====> Loss= 0.007799965   Accuracy= 0.988699973\n",
      "Epoch:  47   =====> Loss= 0.009555947   Accuracy= 0.988799989\n",
      "Epoch:  48   =====> Loss= 0.008459764   Accuracy= 0.989700019\n",
      "Epoch:  49   =====> Loss= 0.008612395   Accuracy= 0.989300013\n",
      "Epoch:  50   =====> Loss= 0.006646191   Accuracy= 0.989899993\n",
      "Epoch:  51   =====> Loss= 0.007647246   Accuracy= 0.991100013\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.contrib.layers import flatten\n",
    "import time\n",
    "\n",
    "def LeNet5_Model_Relu_Drop(data):\n",
    "    data = tf.reshape(data, [-1, 28, 28, 1])\n",
    "    # Set layer1 weights\n",
    "    W_layer1 = weight_variable([5, 5, 1, 6])\n",
    "    b_layer1 = bias_variable([6])\n",
    "    \n",
    "    # Set layer2 weights\n",
    "    W_layer2 = weight_variable([5, 5, 6, 16])\n",
    "    b_layer2 = bias_variable([16])\n",
    "    \n",
    "    # Set layer3 weights\n",
    "    W_layer3 = weight_variable([400,120])\n",
    "    b_layer3 = bias_variable([120])\n",
    "    \n",
    "    # Set layer4 weights\n",
    "    W_layer4 = weight_variable([120,84])\n",
    "    b_layer4 = bias_variable([84])\n",
    "    \n",
    "    # Set layer5 weights\n",
    "    W_layer5 = weight_variable([84,10])\n",
    "    b_layer5 = bias_variable([10])\n",
    "\n",
    "    # Construct model\n",
    "    layer1 = tf.nn.relu(tf.nn.conv2d(data, W_layer1, [1, 1, 1, 1], padding='SAME') + b_layer1)\n",
    "    pooling1 = tf.nn.max_pool(layer1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    layer2 = tf.nn.relu(tf.nn.conv2d(pooling1, W_layer2, [1, 1, 1, 1], padding='VALID') + b_layer2)\n",
    "    pooling2 = tf.nn.max_pool(layer2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    flattened = tf.contrib.layers.flatten(pooling2)\n",
    "    keep_prob = 0.75\n",
    "    layer3 = tf.nn.relu(tf.matmul(flattened, W_layer3) + b_layer3)\n",
    "    layer3_drop = tf.nn.dropout(layer3, keep_prob)\n",
    "    layer4 = tf.nn.relu(tf.matmul(layer3_drop, W_layer4) + b_layer4)\n",
    "    layer4_drop = tf.nn.dropout(layer4, keep_prob)\n",
    "    \n",
    "    with tf.name_scope('Model'):\n",
    "        # Model\n",
    "        model = tf.nn.softmax(tf.matmul(layer4_drop, W_layer5) + b_layer5) # Softmax\n",
    "    return model\n",
    "\n",
    "model_relu_drop = LeNet5_Model_Relu_Drop(x)\n",
    "\n",
    "# Training parameters \n",
    "learning_rate = 0.001\n",
    "training_epochs = 50\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "saving_path = 'models/'\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(model_relu_drop+1e-9), reduction_indices=1))\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    accuracy = evaluate(model_relu_drop, y)\n",
    "with tf.name_scope('Adam'):\n",
    "    # Adam Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()    \n",
    "\n",
    "print(\"***Training with optimal paramiters***\")\n",
    "t0 = time.time()\n",
    "train(learning_rate, training_epochs, batch_size, cost, optimizer)\n",
    "print(\"Time taken: \",time.time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We finally reached the 99.11% accuracy that we were seeking thanks to the tuning of the parameters and by introducing training optimizations like relu activation and the dropout on fully connected layers (even though this last optmization didn't help much)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
